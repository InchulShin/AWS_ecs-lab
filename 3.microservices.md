# ECSWorkshop

## ECS에 마이크로서비스 배포

워크샵에서 무엇을 할 것인가?

우리가 배울 일을 하고 조직에서 작동 할 수와 같은 전형적인 3 계층 수개 국어의 microservice 스택을 구축 할 것입니다.

![](./images/3-tier-architecture.svg)

## 도구 설치 및 구성
Cloud9 작업 공간에서 다음 명령을 실행합니다(참고: 아래 탭에서 하나의 설치 방법을 선택하고 튜토리얼의 나머지 부분에서 동일한 방법을 따릅니다).

### [ecs-cli]
먼저 ECS cli(및 일부 다른 텍스트 유틸리티 포함)를 설치합니다.

```
sudo curl -so /usr/local/bin/ecs-cli https://s3.amazonaws.com/amazon-ecs-cli/ecs-cli-linux-amd64-latest
sudo chmod +x /usr/local/bin/ecs-cli
sudo yum -y install jq gettext
```

다음으로 현재 리전을 기본값으로 사용하여 AWS cli를 구성합니다.

```
export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region)
echo "export AWS_REGION=${AWS_REGION}" >> ~/.bash_profile
aws configure set default.region ${AWS_REGION}
aws configure get default.region
```

## 서비스 저장소 복제
서비스 저장소를 작업 영역에 복제합니다.

```
cd ~/environment
git clone https://github.com/brentley/container-demo
git clone https://github.com/aws-containers/ecsdemo-frontend
git clone https://github.com/brentley/ecsdemo-nodejs
git clone https://github.com/brentley/ecsdemo-crystal
```

## 플랫폼 구축

![](./images/empty-platform.svg)

모놀리스를 마이크로서비스로 나누는 것이 좋은 생각이라면 앱 플랫폼을 관리하는 코드를 작고 단순하게 유지하는 것도 합리적입니다.

이 워크샵에서는 이 리포지토리로 인프라를 관리하고 각 서비스는 자체 별도의 리포지토리에서 유지 관리됩니다.

이 리포지토리는 배포할 마이크로서비스의 기본 환경을 구축하는 데 사용됩니다.

 부조종사-cli 경로
 CDK 경로
 ecs-cli 경로

![](./images/mu-topology-vpc.png)

## 환경 구축

Cloud9 작업 영역에서 다음 명령을 실행합니다.

로드 밸런서 및 ECS에 대한 서비스 연결 역할이 있는지 확인합니다.

```
aws iam get-role --role-name "AWSServiceRoleForElasticLoadBalancing" || aws iam create-service-linked-role --aws-service-name "elasticloadbalancing.amazonaws.com"

aws iam get-role --role-name "AWSServiceRoleForECS" || aws iam create-service-linked-role --aws-service-name "ecs.amazonaws.com"
```

nodejs 및 crystal services를 로드 테스트하는 데 사용할 임시 EC2 인스턴스에 연결할 수 있도록 세션 관리자 플러그인을 cloud9에 설치합니다.

```
curl "https://s3.amazonaws.com/session-manager-downloads/plugin/latest/linux_64bit/session-manager-plugin.rpm" -o "session-manager-plugin.rpm"
sudo yum install -y session-manager-plugin.rpm
session-manager-plugin
```

### CDK
플랫폼 리포지토리로 이동

```
cd ~/environment/container-demo/cdk
```

cdk가 어셈블리 CloudFormation 템플릿을 합성할 수 있는지 확인

```
cdk synth
```

 재미있는 운동! cdk로 작성된 코드와 CloudFormation으로 생성된 총 라인을 비교하기 위해 총 라인 수를 계산해 보겠습니다. 솔루션을 보려면 여기를 확장하십시오.

```
echo -e "Cloudformation Lines==$(cdk synth |wc -l)\nCDK Lines==$(cat app.py|wc -l)"
```

최종 결과는 다음과 같아야 합니다.

```
Cloudformation Lines==470
CDK Lines==82
```

제안된 환경 변경 사항 보기

```
cdk diff
```

환경에 변경 사항 배포

```
cdk deploy --require-approval never
```

무엇을 만들고 있는지 살펴보겠습니다. 스택에 정의된 모든 것이 100% 파이썬 코드로 작성되었음을 알 수 있습니다. 우리는 또한 잘 설계된 관행을 기반으로 구성 요소를 구축할 수 있도록 함으로써 cdk의 독단적인 특성으로부터 이익을 얻습니다. 이것은 또한 리소스(예: 서브넷, nat 게이트웨이 등)를 만들고 연결하기 위해 모든 기본 구성 요소에 대해 생각할 필요가 없다는 것을 의미합니다. cdk 코드를 배포하면 cdk가 기본 Cloudformation 템플릿을 생성하고 배포합니다.

```
# This resource alone will create a private/public subnet in each AZ as well as nat/internet gateway(s)
self.vpc = aws_ec2.Vpc(
    self, "BaseVPC",
    cidr='10.0.0.0/24',
)

# Creating ECS Cluster in the VPC created above
self.ecs_cluster = aws_ecs.Cluster(
    self, "ECSCluster",
    vpc=self.vpc
)

# Adding service discovery namespace to cluster
self.ecs_cluster.add_default_cloud_map_namespace(
    name="service",
)

# Frontend security group frontend service to backend services
self.services_3000_sec_group = aws_ec2.SecurityGroup(
    self, "FrontendToBackendSecurityGroup",
    allow_all_outbound=True,
    description="Security group for frontend service to talk to backend services",
    vpc=self.vpc
)

# Allow inbound 3000 from ALB to Frontend Service
self.sec_grp_ingress_self_3000 = aws_ec2.CfnSecurityGroupIngress(
    self, "InboundSecGrp3000",
    ip_protocol='TCP',
    source_security_group_id=self.services_3000_sec_group.security_group_id,
    from_port=3000,
    to_port=3000,
    group_id=self.services_3000_sec_group.security_group_id
)

# All Outputs required for other stacks to build in the same environment
core.CfnOutput(self, "NSArn", value=self.namespace_outputs['ARN'], export_name="NSARN")
core.CfnOutput(self, "NSName", value=self.namespace_outputs['NAME'], export_name="NSNAME")
core.CfnOutput(self, "NSId", value=self.namespace_outputs['ID'], export_name="NSID")
core.CfnOutput(self, "FE2BESecGrp", value=self.services_3000_sec_group.security_group_id, export_name="SecGrpId")
core.CfnOutput(self, "ECSClusterName", value=self.cluster_outputs['NAME'], export_name="ECSClusterName")
core.CfnOutput(self, "ECSClusterSecGrp", value=self.cluster_outputs['SECGRPS'], export_name="ECSSecGrpList")
core.CfnOutput(self, "ServicesSecGrp", value=self.services_3000_sec_group.security_group_id, export_name="ServicesSecGrp")
```

스택 빌드가 완료되면 기본 CloudFormation 스택에 대한 모든 출력이 인쇄됩니다. 이러한 출력은 마이크로 서비스를 배포할 때 기본 플랫폼을 참조하는 데 사용하는 것입니다. 다음은 출력의 예입니다.

```
   ecsworkshop-base

Outputs:
ecsworkshop-base.NSName = service
ecsworkshop-base.NSId = ns-jxsmy6sggusms4vr
ecsworkshop-base.ECSClusterName = ecsworkshop-base-ECSCluster7D463CD4-123JC9IHENY94
ecsworkshop-base.FE2BESecGrp = sg-0681f217a4d567ece
ecsworkshop-base.NSArn = arn:aws:servicediscovery:us-west-2:123456789:namespace/ns-jxsmy6sggusms4vr
ecsworkshop-base.ServicesSecGrp = sg-0681f217a4d567ece
ecsworkshop-base.ECSClusterSecGrp = []

Stack ARN:
arn:aws:cloudformation:us-west-2:123456789:stack/ecsworkshop-base/afe381b0-58e1-11ea-8997-02e1301110e6
```

### ecs-cli Fargate 모드
VPC, ECS 클러스터 및 ALB를 구축합니다.

```
cd ~/environment/container-demo

aws cloudformation deploy --stack-name container-demo --template-file cluster-fargate-private-vpc.yml --capabilities CAPABILITY_IAM

aws cloudformation deploy --stack-name container-demo-alb --template-file alb-external.yml
```

### ecs-cli ec2 모드
VPC, ECS 클러스터 및 ALB를 구축합니다.

```
cd ~/environment/container-demo

aws cloudformation deploy --stack-name container-demo --template-file cluster-ec2-private-vpc.yml --capabilities CAPABILITY_IAM

aws cloudformation deploy --stack-name container-demo-alb --template-file alb-external.yml
```

## 프론트엔드 레일즈 앱

### CDK
배포 구성 확인

```
cd ~/environment/ecsdemo-frontend/cdk
```

cdk가 어셈블리 CloudFormation 템플릿을 합성할 수 있는지 확인

```
cdk synth
```

cdk가 환경에서 구축 및/또는 변경을 제안하는 내용을 검토합니다.

```
cdk diff
```

프런트엔드 웹 서비스 배포

```
cdk deploy --require-approval never
```

배포가 완료되면 두 개의 출력이 표시됩니다. 프런트엔드 URL 출력을 찾아 새 탭에서 해당 링크를 엽니다. 이 시점에서 프론트엔드 웹사이트가 실행되고 있는 것을 볼 수 있습니다. 다음은 예제 출력입니다.

![](./images/frontend-output.png)

코드 검토
플랫폼 빌드에서 언급했듯이 코드를 통해 배포 구성을 정의합니다. cdk가 배포되는 방식을 더 잘 이해하기 위해 코드를 살펴보겠습니다.

기본 플랫폼 스택에서 기본 구성 값 가져오기
자체 스택에 플랫폼을 구축했기 때문에 배포되는 모든 서비스에서 재사용해야 하는 특정 환경적 가치가 있습니다. 이 사용자 지정 구성에서는 기본 플랫폼 스택에서 VPC, ECS 클러스터 및 Cloud Map 네임스페이스를 가져옵니다. 이를 사용자 정의 구성으로 래핑하여 서비스 배포 논리에서 플랫폼 가져오기를 분리합니다.

```
class BasePlatform(core.Construct):
    
    def __init__(self, scope: core.Construct, id: str, **kwargs):
        super().__init__(scope, id, **kwargs)

        # The base platform stack is where the VPC was created, so all we need is the name to do a lookup and import it into this stack for use
        self.vpc = aws_ec2.Vpc.from_lookup(
            self, "ECSWorkshopVPC",
            vpc_name='ecsworkshop-base/BaseVPC'
        )
        
        # Importing the service discovery namespace from the base platform stack
        self.sd_namespace = aws_servicediscovery.PrivateDnsNamespace.from_private_dns_namespace_attributes(
            self, "SDNamespace",
            namespace_name=core.Fn.import_value('NSNAME'),
            namespace_arn=core.Fn.import_value('NSARN'),
            namespace_id=core.Fn.import_value('NSID')
        )
        
        # Importing the ECS cluster from the base platform stack
        self.ecs_cluster = aws_ecs.Cluster.from_cluster_attributes(
            self, "ECSCluster",
            cluster_name=core.Fn.import_value('ECSClusterName'),
            security_groups=[],
            vpc=self.vpc,
            default_cloud_map_namespace=self.sd_namespace
        )

        # Importing the security group that allows frontend to communicate with backend services
        self.services_sec_grp = aws_ec2.SecurityGroup.from_security_group_id(
            self, "ServicesSecGrp",
            security_group_id=core.Fn.import_value('ServicesSecGrp')
        )
```

프런트엔드 서비스 배포 코드
프론트엔드 서비스의 경우 프론트엔드 서비스로 제공하기 위해 구축해야 하는 구성 요소가 꽤 있습니다. 이러한 구성 요소는 Application Load Balancer, 대상 그룹, ECS 작업 정의 및 ECS 서비스입니다. 이러한 구성 요소를 자체적으로 구축하는 것은 수백 줄의 CloudFormation과 동일하지만 cdk가 제공하는 더 높은 수준의 구성을 사용하면 18줄의 코드로 모든 것을 구축할 수 있습니다.

```
class FrontendService(core.Stack):
    
    def __init__(self, scope: core.Stack, id: str, **kwargs):
        super().__init__(scope, id, **kwargs)

        self.base_platform = BasePlatform(self, self.stack_name)

        # This defines some of the components required for the docker container to run
        self.fargate_task_image = aws_ecs_patterns.ApplicationLoadBalancedTaskImageOptions(
            image=aws_ecs.ContainerImage.from_registry("brentley/ecsdemo-frontend"),
            container_port=3000,
            environment={
                "CRYSTAL_URL": "http://ecsdemo-crystal.service:3000/crystal",
                "NODEJS_URL": "http://ecsdemo-nodejs.service:3000"
            },
        )

        # This high level construct will build everything required to ensure our container is load balanced and running as an ECS service
        self.fargate_load_balanced_service = aws_ecs_patterns.ApplicationLoadBalancedFargateService(
            self, "FrontendFargateLBService",
            cluster=self.base_platform.ecs_cluster,
            cpu=256,
            memory_limit_mib=512,
            desired_count=1,
            public_load_balancer=True,
            cloud_map_options=self.base_platform.sd_namespace,
            task_image_options=self.fargate_task_image
        )

        # Utilizing the connections method to connect the frontend service security group to the backend security group
        self.fargate_load_balanced_service.service.connections.allow_to(
            self.base_platform.services_sec_grp,
            port_range=aws_ec2.Port(protocol=aws_ec2.Protocol.TCP, string_representation="frontendtobackend", from_port=3000, to_port=3000)
        )
```

서비스 로그 검토
명령줄에서 서비스 로그를 검토합니다.
 솔루션을 보려면 여기를 확장하십시오.

먼저 cdk가 우리를 대신하여 로그 그룹을 생성했기 때문에 서비스 이름을 기반으로 로그 그룹의 이름을 가져와야 합니다. 다음으로 터미널에서 CloudWatch의 활성 로그를 추적합니다. 우리는 이라는 오픈 소스 도구를 사용하여 이를 달성합니다.awslogs.

```
log_group=$(awslogs groups -p ecsworkshop-frontend)
awslogs get -G -S --timestamp --start 1m --watch $log_group
```

다음은 출력의 예입니다.

![](./images/cli-logs.gif)

콘솔에서 서비스 로그를 검토합니다.
 솔루션을 보려면 여기를 확장하십시오.

먼저 콘솔에서 ECS로 이동하고 서비스를 드릴다운하여 자세한 정보를 얻습니다. 보시다시피 로드 밸런서 세부 정보, 실행 중인 작업 수 및 로그와 같이 서비스 자체에 대해 수집할 수 있는 많은 정보가 있습니다. 실행 중인 서비스에 대한 로그를 검토하려면 로그 탭을 클릭합니다.

![](./images/ecs-console-service-logs.gif)

다음으로 거의 실시간으로 서비스 로그를 검토할 수 있습니다. 1주일까지 시간을 되돌리거나 지난 30초로 드릴다운할 수 있습니다. 아래 예에서는 30초를 선택합니다.

![](./images/ecs-console-logs.gif)

서비스 확장
수동 확장
 솔루션을 보려면 여기를 확장하십시오.

서비스를 수동으로 확장하려면 다음에서 코드를 수정하면 됩니다. app.py 원하는 개수를 1에서 3으로 변경

```
self.fargate_load_balanced_service = aws_ecs_patterns.ApplicationLoadBalancedFargateService(
    self, "FrontendFargateLBService",
    cluster=self.base_platform.ecs_cluster,
    cpu=256,
    memory_limit_mib=512,
    desired_count=3,
    #desired_count=1,
    public_load_balancer=True,
    cloud_map_options=self.base_platform.sd_namespace,
    task_image_options=self.fargate_task_image
)
```

코드를 업데이트했으면 변경 사항이 적용되는지 확인하겠습니다.

```
cdk diff
```

출력은 다음과 같아야 합니다.

```
diff-service-count
```

변경 사항이 좋아 보이는 것을 확인했으므로 이제 배포해 보겠습니다.

```
cdk deploy
```

출력은 다음과 같아야 합니다.

![](./images/cdk-deploy-service-count.png)



자동 확장
 솔루션을 보려면 여기를 확장하십시오.

자동 크기 조정이 필요한 이유
간단히 말해서 - 그것은 인간이 서비스를 확장하거나 오케스트레이터입니다.
수동으로 수행하도록 선택하면 로드가 증가함에 따라 로드에 맞게 서비스를 확장하기 위해 수행 중인 작업을 중지해야 합니다(로드가 해결되면 결국 다시 축소해야 함은 말할 것도 없습니다). 이것은 지루하고 고통스러울 수 있으므로 자동 크기 조정이 존재하는 이유입니다.
오케스트레이터가 서비스의 확장 및 축소를 처리하도록 하면 운영상의 부담을 줄이고 지속적인 개선에 집중할 수 있습니다. 자동 크기 조정 설정을 얻으려면 먼저 자동 크기 조정 결정으로 사용할 측정항목을 알아야 합니다. 조정에 대한 몇 가지 예시적인 메트릭은 CPU 사용률, 메모리 사용률 및 대기열 깊이입니다.
코드에서 자동 크기 조정 설정
선택한 편집기를 사용하여 cdk 디렉토리에서 app.py를 엽니다.

검색 Enable Service Autoscaling 서비스에 대한 자동 크기 조정을 활성화할 코드를 찾습니다.

self.autoscale 이하 코드에서 주석(#)을 제거합니다. 제거하면 다음과 같이 표시됩니다.

```
# Enable Service Autoscaling
self.autoscale = self.fargate_load_balanced_service.service.auto_scale_task_count(
    min_capacity=1,
    max_capacity=10
)

self.autoscale.scale_on_cpu_utilization(
    "CPUAutoscaling",
    target_utilization_percent=50,
    scale_in_cooldown=core.Duration.seconds(30),
    scale_out_cooldown=core.Duration.seconds(30)
)
```

코드 검토
자동 크기 조정 논리 모델링을 시작하려면 먼저 상한과 하한을 설정합니다. 이것은 우리가 항상 최소 1개의 작업과 최대 10개의 작업에 있음을 보장합니다.

```
# Enable Service Autoscaling
self.autoscale = self.fargate_load_balanced_service.service.auto_scale_task_count(
    min_capacity=1,
    max_capacity=10
)
```

ECS 서비스가 배포되면 CPU 사용률과 같은 Cloudwatch 지표가 기본적으로 활성화됩니다. 우리는 이 메트릭을 활용하여 확장 목표로 사용할 것입니다. 이 방법에서 우리는 목표 CPU 사용률이 얼마인지, 그리고 스케일 활동 사이에 다른 작업을 추가/제거하기 전에 대기할 시간을 설정하고 있습니다.

```
self.autoscale.scale_on_cpu_utilization(
    "CPUAutoscaling",
    target_utilization_percent=50,
    scale_in_cooldown=core.Duration.seconds(30),
    scale_out_cooldown=core.Duration.seconds(30)
)
```

자동 확장 배포
이제 자동 크기 조정 코드가 준비되었으므로 배포해 보겠습니다.

현재 상태와 제안된 환경 변경 사항의 차이점을 살펴보겠습니다. 다음을 실행합니다.

```
cdk diff
```

두 개의 리소스가 추가된 것을 볼 수 있습니다(아래 이미지). ECS는 Application Autoscaling 서비스를 활용하여 ECS 작업의 확장을 관리하고 있습니다. 요컨대, 이것은 만들 것입니다대상 추적 정책, 확장 및 축소에 대한 원하는 대상(이 경우 CPU 사용률)을 설정하고 ECS 서비스에 연결합니다.

![](./images/task-autoscaling-example.png)

배포 시간!

```
cdk deploy --require-approval never
```

부하 테스트
다음으로 프론트엔드에 로드를 생성해 보겠습니다.

```
alb_url=$(aws cloudformation describe-stacks --stack-name ecsworkshop-frontend --query "Stacks" --output json | jq -r '.[].Outputs[] | select(.OutputKey |contains("LoadBalancer")) | .OutputValue')
siege -c 20 -i $alb_url&
```

siege가 백그라운드에서 실행되는 동안 콘솔로 이동하거나 명령줄에서 자동 크기 조정을 모니터링합니다

 명령줄

실행 중인 작업과 원하는 작업을 비교합니다. 프론트엔드 서비스의 로드가 증가함에 따라 이러한 수가 결국 10까지 증가하는 것을 볼 수 있습니다. 이것은 실시간으로 발생하는 자동 크기 조정입니다. 이 단계는 몇 분 정도 소요됩니다. 한 터미널에서 자유롭게 실행하고 다른 터미널에서 다음 단계로 이동하십시오.

```
while true; do sleep 3; aws ecs describe-services --cluster container-demo --services ecsdemo-frontend | jq '.services[] | "Tasks Desired: \(.desiredCount) vs Tasks Running: \(.runningCount)"'; done 
```

이제 서비스가 자동 확장되는 것을 보았으므로 실행 중인 while 루프를 중지하겠습니다. 누르기만 하면control + c 취소합니다.

부하 테스트를 취소할 시간입니다. 명령 앞에 &, 우리는 백그라운드에서 실행하도록 지시했습니다. 전경으로 다시 가져와 중지합니다. 중지하려면 다음을 입력하십시오.

fg
control + c
참고: 애플리케이션 가용성을 보장하기 위해 서비스는 가능한 한 빨리 메트릭에 비례하여 확장되지만 더 점진적으로 축소됩니다. 자세한 내용은선적 서류 비치

 콘솔

곧!

### ecs-cli Fargate 모드
빌드에서 환경 변수 설정

```
export clustername=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`ClusterName`].OutputValue' --output text)

export target_group_arn=$(aws cloudformation describe-stack-resources --stack-name container-demo-alb | jq -r '.[][] | select(.ResourceType=="AWS::ElasticLoadBalancingV2::TargetGroup").PhysicalResourceId')

export vpc=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`VpcId`].OutputValue' --output text)

export ecsTaskExecutionRole=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`ECSTaskExecutionRole`].OutputValue' --output text)

export subnet_1=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`PrivateSubnetOne`].OutputValue' --output text)

export subnet_2=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`PrivateSubnetTwo`].OutputValue' --output text)

export subnet_3=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`PrivateSubnetThree`].OutputValue' --output text)

export security_group=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`ContainerSecurityGroup`].OutputValue' --output text)

cd ~/environment
```

구성 ecs-cli 클러스터와 대화하려면 다음 안내를 따르세요.

```
ecs-cli configure --region $AWS_REGION --cluster $clustername --default-launch-type FARGATE --config-name container-demo
```

명령을 실행할 때 해당 지역을 참조할 수 있도록 기본 지역을 설정합니다.

트래픽 승인:

```
aws ec2 authorize-security-group-ingress --group-id "$security_group" --protocol tcp --port 3000 --source-group "$security_group"
```

컨테이너가 포트 3000에서 통신한다는 것을 알고 있으므로 보안 그룹에서 해당 트래픽을 승인합니다.

프론트엔드 애플리케이션 배포:

```
cd ~/environment/ecsdemo-frontend
envsubst < ecs-params.yml.template >ecs-params.yml

ecs-cli compose --project-name ecsdemo-frontend service up \
    --create-log-groups \
    --target-group-arn $target_group_arn \
    --private-dns-namespace service \
    --enable-service-discovery \
    --container-name ecsdemo-frontend \
    --container-port 3000 \
    --cluster-config container-demo \
    --vpc $vpc
```

여기에서 디렉토리를 프론트엔드 애플리케이션 코드 디렉토리로 변경합니다. NSenvsubst 명령 템플릿 ecs-params.yml현재 값으로 파일을 만듭니다. 그런 다음 ECS 클러스터에서 프론트엔드 서비스를 시작합니다(기본 시작 유형은 Fargate).

참고: ecs-cli는 서비스 검색을 위한 프라이빗 dns 네임스페이스 및 Cloudwatch 로그의 로그 그룹 구축을 처리합니다.

실행 중인 컨테이너를 보고 나중에 사용할 수 있도록 작업 ID의 출력을 env 변수로 저장합니다.

```
ecs-cli compose --project-name ecsdemo-frontend service ps \
    --cluster-config container-demo

task_id=$(ecs-cli compose --project-name ecsdemo-frontend service ps --cluster-config container-demo | awk -F \/ 'FNR == 2 {print $2}')
```

하나의 작업을 등록해야 합니다.

연결 가능성 확인(브라우저에서 URL 열기):

```
alb_url=$(aws cloudformation describe-stacks --stack-name container-demo-alb --query 'Stacks[0].Outputs[?OutputKey==`ExternalUrl`].OutputValue' --output text)
echo "Open $alb_url in your browser"
```

이 명령은 수신 ALB의 URL을 조회하고 출력합니다. 클릭하여 열거나 브라우저에 복사하여 붙여넣을 수 있어야 합니다.

로그 보기:

```
# Referencing task id from above ps command
ecs-cli logs --task-id $task_id \
    --follow --cluster-config container-demo
```

로그를 보려면 이전 ps명령 에서 작업 ID를 찾아 이 명령에서 사용합니다. 작업의 로그를 따를 수도 있습니다.

작업 확장:

```
ecs-cli compose --project-name ecsdemo-frontend service scale 3 \
    --cluster-config container-demo
ecs-cli compose --project-name ecsdemo-frontend service ps \
    --cluster-config container-demo
```

이제 컨테이너가 3개의 가용 영역 모두에 고르게 분산되었음을 알 수 있습니다.

### ecs-cli ec2 모드

빌드에서 환경 변수 설정

```
export clustername=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`ClusterName`].OutputValue' --output text)
export target_group_arn=$(aws cloudformation describe-stack-resources --stack-name container-demo-alb | jq -r '.[][] | select(.ResourceType=="AWS::ElasticLoadBalancingV2::TargetGroup").PhysicalResourceId')
export vpc=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`VpcId`].OutputValue' --output text)
export ecsTaskExecutionRole=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`ECSTaskExecutionRole`].OutputValue' --output text)
export subnet_1=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`PrivateSubnetOne`].OutputValue' --output text)
export subnet_2=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`PrivateSubnetTwo`].OutputValue' --output text)
export subnet_3=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`PrivateSubnetThree`].OutputValue' --output text)
export security_group=$(aws cloudformation describe-stacks --stack-name container-demo --query 'Stacks[0].Outputs[?OutputKey==`ContainerSecurityGroup`].OutputValue' --output text)

cd ~/environment
```

구성 ecs-cli 클러스터와 대화하려면 다음 안내를 따르세요.

```
ecs-cli configure --region $AWS_REGION --cluster $clustername --default-launch-type EC2 --config-name container-demo
```

명령을 실행할 때 해당 지역을 참조할 수 있도록 기본 지역을 설정합니다.

트래픽 승인:

```
aws ec2 authorize-security-group-ingress --group-id "$security_group" --protocol tcp --port 3000 --source-group "$security_group"
```

컨테이너가 포트 3000에서 통신한다는 것을 알고 있으므로 보안 그룹에서 해당 트래픽을 승인합니다.

프론트엔드 애플리케이션 배포:

```
cd ~/environment/ecsdemo-frontend
envsubst < ecs-params.yml.template >ecs-params.yml

ecs-cli compose --project-name ecsdemo-frontend service up \
    --create-log-groups \
    --target-group-arn $target_group_arn \
    --private-dns-namespace service \
    --enable-service-discovery \
    --container-name ecsdemo-frontend \
    --container-port 3000 \
    --cluster-config container-demo \
    --vpc $vpc
```

여기에서 디렉토리를 프론트엔드 애플리케이션 코드 디렉토리로 변경합니다. NSenvsubst 명령 템플릿 ecs-params.yml현재 값으로 파일을 만듭니다. 그런 다음 ECS 클러스터에서 프론트엔드 서비스를 시작합니다(기본 시작 유형은 Fargate).

참고: ecs-cli는 서비스 검색을 위한 프라이빗 dns 네임스페이스 및 Cloudwatch 로그의 로그 그룹 구축을 처리합니다.

실행 중인 컨테이너를 보고 나중에 사용할 수 있도록 작업 ID의 출력을 env 변수로 저장합니다.

```
ecs-cli compose --project-name ecsdemo-frontend service ps \
    --cluster-config container-demo

task_id=$(ecs-cli compose --project-name ecsdemo-frontend service ps --cluster-config container-demo | awk -F \/ 'FNR == 2 {print $2}')
```

하나의 작업을 등록해야 합니다.

연결 가능성 확인(브라우저에서 URL 열기):

```
alb_url=$(aws cloudformation describe-stacks --stack-name container-demo-alb --query 'Stacks[0].Outputs[?OutputKey==`ExternalUrl`].OutputValue' --output text)
echo "Open $alb_url in your browser"
```

이 명령은 수신 ALB의 URL을 조회하고 출력합니다. 클릭하여 열거나 브라우저에 복사하여 붙여넣을 수 있어야 합니다.

로그 보기:

```
# Referencing task id from above ps command
ecs-cli logs --task-id $task_id \
    --follow --cluster-config container-demo
```

로그를 보려면 이전 ps명령 에서 작업 ID를 찾아 이 명령에서 사용합니다. 작업의 로그를 따를 수도 있습니다.

작업 확장:

```
ecs-cli compose --project-name ecsdemo-frontend service scale 3 \
    --cluster-config container-demo
ecs-cli compose --project-name ecsdemo-frontend service ps \
    --cluster-config container-demo
```

이제 컨테이너가 3개의 가용 영역 모두에 고르게 분산되었음을 알 수 있습니다.

## NODE.JS 백엔드 API

### CDK

배포 구성 확인

```
cd ~/environment/ecsdemo-nodejs/cdk
```

cdk가 어셈블리 CloudFormation 템플릿을 합성할 수 있는지 확인

```
cdk synth
```

cdk가 환경에서 구축 및/또는 변경을 제안하는 내용을 검토합니다.

```
cdk diff
```

Nodejs 백엔드 서비스 배포

```
cdk deploy --require-approval never
```

코드 검토
플랫폼 빌드에서 언급했듯이 코드를 통해 배포 구성을 정의합니다. cdk가 배포되는 방식을 더 잘 이해하기 위해 코드를 살펴보겠습니다.

기본 플랫폼 스택에서 기본 구성 값 가져오기
자체 스택에 플랫폼을 구축했기 때문에 배포되는 모든 서비스에서 재사용해야 하는 특정 환경적 가치가 있습니다. 이 사용자 지정 구성에서는 기본 플랫폼 스택에서 VPC, ECS 클러스터 및 Cloud Map 네임스페이스를 가져옵니다. 이를 사용자 정의 구성으로 래핑하여 서비스 배포 논리에서 플랫폼 가져오기를 분리합니다.

```
class BasePlatform(core.Construct):
    
    def __init__(self, scope: core.Construct, id: str, **kwargs):
        super().__init__(scope, id, **kwargs)

        # The base platform stack is where the VPC was created, so all we need is the name to do a lookup and import it into this stack for use
        self.vpc = aws_ec2.Vpc.from_lookup(
            self, "ECSWorkshopVPC",
            vpc_name='ecsworkshop-base/BaseVPC'
        )
        
        # Importing the service discovery namespace from the base platform stack
        self.sd_namespace = aws_servicediscovery.PrivateDnsNamespace.from_private_dns_namespace_attributes(
            self, "SDNamespace",
            namespace_name=core.Fn.import_value('NSNAME'),
            namespace_arn=core.Fn.import_value('NSARN'),
            namespace_id=core.Fn.import_value('NSID')
        )
        
        # Importing the ECS cluster from the base platform stack
        self.ecs_cluster = aws_ecs.Cluster.from_cluster_attributes(
            self, "ECSCluster",
            cluster_name=core.Fn.import_value('ECSClusterName'),
            security_groups=[],
            vpc=self.vpc,
            default_cloud_map_namespace=self.sd_namespace
        )

        # Importing the security group that allows frontend to communicate with backend services
        self.services_sec_grp = aws_ec2.SecurityGroup.from_security_group_id(
            self, "ServicesSecGrp",
            security_group_id=core.Fn.import_value('ServicesSecGrp')
        )
```

Nodejs 백엔드 서비스 배포 코드
백엔드 서비스의 경우 도커 이미지에서 컨테이너를 실행하고 싶지만 이를 배포하고 스케줄러 뒤에 가져오는 방법을 파악해야 합니다. 이를 자체적으로 수행하려면 작업 정의, ECS 서비스를 구축하고 서비스 검색을 위해 CloudMap 뒤에서 이를 가져오는 방법을 알아내야 합니다. 이러한 구성 요소를 자체적으로 구축하는 것은 수백 줄의 CloudFormation과 동일하지만 cdk가 제공하는 더 높은 수준의 구성을 사용하면 30줄의 코드로 모든 것을 구축할 수 있습니다.

```
class NodejsService(core.Stack):
    
    def __init__(self, scope: core.Stack, id: str, **kwargs):
        super().__init__(scope, id, **kwargs)

        # Importing our shared values from the base stack construct
        self.base_platform = BasePlatform(self, self.stack_name)

        # The task definition is where we store details about the task that will be scheduled by the service
        self.fargate_task_def = aws_ecs.TaskDefinition(
            self, "TaskDef",
            compatibility=aws_ecs.Compatibility.EC2_AND_FARGATE,
            cpu='256',
            memory_mib='512',
        )
        
        # The container definition defines the container(s) to be run when the task is instantiated
        self.container = self.fargate_task_def.add_container(
            "NodeServiceContainerDef",
            image=aws_ecs.ContainerImage.from_registry("brentley/ecsdemo-nodejs"),
            memory_reservation_mib=512,
            logging=aws_ecs.LogDriver.aws_logs(
                stream_prefix='ecsworkshop-nodejs'
            )
        )
        
        # Serve this container on port 3000
        self.container.add_port_mappings(
            aws_ecs.PortMapping(
                container_port=3000
            )
        )

        # Build the service definition to schedule the container in the shared cluster
        self.fargate_service = aws_ecs.FargateService(
            self, "NodejsFargateService",
            task_definition=self.fargate_task_def,
            cluster=self.base_platform.ecs_cluster,
            security_group=self.base_platform.services_sec_grp,
            desired_count=1,
            cloud_map_options=aws_ecs.CloudMapOptions(
                cloud_map_namespace=self.base_platform.sd_namespace,
                name='ecsdemo-nodejs'
            )
        )
```

서비스 로그 검토
명령줄에서 서비스 로그를 검토합니다.
 솔루션을 보려면 여기를 확장하십시오.

먼저 cdk가 우리를 대신하여 로그 그룹을 생성했기 때문에 서비스 이름을 기반으로 로그 그룹의 이름을 가져와야 합니다. 다음으로 터미널에서 CloudWatch의 활성 로그를 추적합니다. 우리는 이라는 오픈 소스 도구를 사용하여 이를 달성합니다.awslogs.

```
log_group=$(awslogs groups -p ecsworkshop-nodejs)
awslogs get -G -S --timestamp --start 1m --watch $log_group
```

다음은 출력의 예입니다.

![](./images/cli-logs (1).gif)

콘솔에서 서비스 로그를 검토합니다.
 솔루션을 보려면 여기를 확장하십시오.

먼저 콘솔에서 ECS로 이동하고 서비스를 드릴다운하여 자세한 정보를 얻습니다. 보시다시피 서비스 검색 세부 정보, 실행 중인 작업 수 및 로그와 같이 서비스 자체에 대해 수집할 수 있는 많은 정보가 있습니다. 실행 중인 서비스에 대한 로그를 검토하려면 로그 탭을 클릭합니다.

![](./images/ecs-console-service-logs (1).gif)

다음으로 거의 실시간으로 서비스 로그를 검토할 수 있습니다. 1주일까지 시간을 되돌리거나 지난 30초로 드릴다운할 수 있습니다. 아래 예에서는 30초를 선택합니다.콘솔 서비스 

![](./images/ecs-console-logs (1).gif)

서비스 확장
수동 확장
 솔루션을 보려면 여기를 확장하십시오.

서비스를 수동으로 확장하려면 다음에서 코드를 수정하면 됩니다. app.py 원하는 개수를 1에서 3으로 변경

```
self.fargate_service = aws_ecs.FargateService(
    self, "NodejsFargateService",
    task_definition=self.fargate_task_def,
    cluster=self.base_platform.ecs_cluster,
    security_group=self.base_platform.services_sec_grp,
    desired_count=3,
    #desired_count=1,
    cloud_map_options=aws_ecs.CloudMapOptions(
        cloud_map_namespace=self.base_platform.sd_namespace,
        name='ecsdemo-nodejs'
    )
)
```

코드를 업데이트했으면 변경 사항이 적용되는지 확인하겠습니다.

```
cdk diff
```

출력은 다음과 같아야 합니다.

![](./images/cdk-service-count-diff.png)

변경 사항이 좋아 보이는 것을 확인했으므로 이제 배포해 보겠습니다.

```
cdk deploy
```

출력은 다음과 같아야 합니다.

![](./images/cdk-deploy-service-count (1).png)

자동 확장
 솔루션을 보려면 여기를 확장하십시오.

곧!

### ecs-cli ec2 모드

NodeJS 백엔드 API를 불러오자!

NodeJS 백엔드 애플리케이션 배포:

```
cd ~/environment/ecsdemo-nodejs
envsubst < ecs-params.yml.template >ecs-params.yml

ecs-cli compose --project-name ecsdemo-nodejs service up \
    --create-log-groups \
    --private-dns-namespace service \
    --enable-service-discovery \
    --cluster-config container-demo \
    --vpc $vpc
```

여기에서 디렉토리를 nodejs 애플리케이션 코드 디렉토리로 변경합니다. NSenvsubst 명령 템플릿 ecs-params.yml현재 값으로 파일을 만듭니다. 그런 다음 ECS 클러스터에서 nodejs 서비스를 시작합니다(기본 시작 유형은 Fargate).

참고: ecs-cli는 서비스 검색을 위한 프라이빗 dns 네임스페이스 및 Cloudwatch 로그의 로그 그룹 구축을 처리합니다.

실행 중인 컨테이너를 보고 나중에 사용할 수 있도록 작업 ID의 출력을 env 변수로 저장합니다.

```
ecs-cli compose --project-name ecsdemo-nodejs service ps \
    --cluster-config container-demo

task_id=$(ecs-cli compose --project-name ecsdemo-nodejs service ps --cluster-config container-demo | awk -F \/ 'FNR == 2 {print $2}')
```

하나의 작업을 등록해야 합니다.

연결 가능성 확인(브라우저에서 URL 열기):

```
alb_url=$(aws cloudformation describe-stacks --stack-name container-demo-alb --query 'Stacks[0].Outputs[?OutputKey==`ExternalUrl`].OutputValue' --output text)
echo "Open $alb_url in your browser"
```

이 명령은 수신 ALB의 URL을 조회하고 출력합니다. 클릭하여 열거나 브라우저에 복사하여 붙여넣을 수 있어야 합니다.

로그 보기:

```
# Referencing task id from above ps command
ecs-cli logs --task-id $task_id \
    --follow --cluster-config container-demo
```

로그를 보려면 이전 ps명령 에서 작업 ID를 찾아 이 명령에서 사용합니다. 작업의 로그를 따를 수도 있습니다.

작업 확장:

```
ecs-cli compose --project-name ecsdemo-nodejs service scale 3 \
    --cluster-config container-demo
ecs-cli compose --project-name ecsdemo-nodejs service ps \
    --cluster-config container-demo
```

이제 컨테이너가 3개의 가용 영역 모두에 고르게 분산되었음을 알 수 있습니다.

## 크리스탈 백엔드 API

### CDK

배포 구성 확인

```
cd ~/environment/ecsdemo-crystal/cdk
```

cdk가 어셈블리 CloudFormation 템플릿을 합성할 수 있는지 확인

```
cdk synth
```

cdk가 환경에서 구축 및/또는 변경을 제안하는 내용을 검토합니다.

```
cdk diff
```

Nodejs 백엔드 서비스 배포

```
cdk deploy --require-approval never
```

코드 검토
플랫폼 빌드에서 언급했듯이 코드를 통해 배포 구성을 정의합니다. cdk가 배포되는 방식을 더 잘 이해하기 위해 코드를 살펴보겠습니다.

기본 플랫폼 스택에서 기본 구성 값 가져오기
자체 스택에 플랫폼을 구축했기 때문에 배포되는 모든 서비스에서 재사용해야 하는 특정 환경적 가치가 있습니다. 이 사용자 지정 구성에서는 기본 플랫폼 스택에서 VPC, ECS 클러스터 및 Cloud Map 네임스페이스를 가져옵니다. 이를 사용자 정의 구성으로 래핑하여 서비스 배포 논리에서 플랫폼 가져오기를 분리합니다.

```
class BasePlatform(core.Construct):
    
    def __init__(self, scope: core.Construct, id: str, **kwargs):
        super().__init__(scope, id, **kwargs)

        # The base platform stack is where the VPC was created, so all we need is the name to do a lookup and import it into this stack for use
        self.vpc = aws_ec2.Vpc.from_lookup(
            self, "ECSWorkshopVPC",
            vpc_name='ecsworkshop-base/BaseVPC'
        )
        
        # Importing the service discovery namespace from the base platform stack
        self.sd_namespace = aws_servicediscovery.PrivateDnsNamespace.from_private_dns_namespace_attributes(
            self, "SDNamespace",
            namespace_name=core.Fn.import_value('NSNAME'),
            namespace_arn=core.Fn.import_value('NSARN'),
            namespace_id=core.Fn.import_value('NSID')
        )
        
        # Importing the ECS cluster from the base platform stack
        self.ecs_cluster = aws_ecs.Cluster.from_cluster_attributes(
            self, "ECSCluster",
            cluster_name=core.Fn.import_value('ECSClusterName'),
            security_groups=[],
            vpc=self.vpc,
            default_cloud_map_namespace=self.sd_namespace
        )

        # Importing the security group that allows frontend to communicate with backend services
        self.services_sec_grp = aws_ec2.SecurityGroup.from_security_group_id(
            self, "ServicesSecGrp",
            security_group_id=core.Fn.import_value('ServicesSecGrp')
        )
```

Crystal 백엔드 서비스 배포 코드
백엔드 서비스의 경우 도커 이미지에서 컨테이너를 실행하고 싶지만 이를 배포하고 스케줄러 뒤에 가져오는 방법을 파악해야 합니다. 이를 자체적으로 수행하려면 작업 정의, ECS 서비스를 구축하고 서비스 검색을 위해 CloudMap 뒤에서 이를 가져오는 방법을 알아내야 합니다. 이러한 구성 요소를 자체적으로 구축하는 것은 수백 줄의 CloudFormation과 동일하지만 cdk가 제공하는 더 높은 수준의 구성을 사용하면 30줄의 코드로 모든 것을 구축할 수 있습니다.

```
class CrystalService(core.Stack):
    
    def __init__(self, scope: core.Stack, id: str, **kwargs):
        super().__init__(scope, id, **kwargs)

        # Importing our shared values from the base stack construct
        self.base_platform = BasePlatform(self, self.stack_name)

        # The task definition is where we store details about the task that will be scheduled by the service
        self.fargate_task_def = aws_ecs.TaskDefinition(
            self, "TaskDef",
            compatibility=aws_ecs.Compatibility.EC2_AND_FARGATE,
            cpu='256',
            memory_mib='512',
        )
        
        # The container definition defines the container(s) to be run when the task is instantiated
        self.container = self.fargate_task_def.add_container(
            "CrystalServiceContainerDef",
            image=aws_ecs.ContainerImage.from_registry("brentley/ecsdemo-crystal"),
            memory_reservation_mib=512,
            logging=aws_ecs.LogDriver.aws_logs(
                stream_prefix='ecsworkshop-crystal'
            )
        )
        
        # Serve this container on port 3000
        self.container.add_port_mappings(
            aws_ecs.PortMapping(
                container_port=3000
            )
        )

        # Build the service definition to schedule the container in the shared cluster
        self.fargate_service = aws_ecs.FargateService(
            self, "CrystalFargateService",
            task_definition=self.fargate_task_def,
            cluster=self.base_platform.ecs_cluster,
            security_group=self.base_platform.services_sec_grp,
            desired_count=1,
            cloud_map_options=aws_ecs.CloudMapOptions(
                cloud_map_namespace=self.base_platform.sd_namespace,
                name='ecsdemo-crystal'
            )
        )
```

서비스 로그 검토
명령줄에서 서비스 로그를 검토합니다.
 솔루션을 보려면 여기를 확장하십시오.

먼저 cdk가 우리를 대신하여 로그 그룹을 생성했기 때문에 서비스 이름을 기반으로 로그 그룹의 이름을 가져와야 합니다. 다음으로 터미널에서 CloudWatch의 활성 로그를 추적합니다. 우리는 이라는 오픈 소스 도구를 사용하여 이를 달성합니다.awslogs.

```
log_group=$(awslogs groups -p ecsworkshop-crystal)
awslogs get -G -S --timestamp --start 1m --watch $log_group
```

다음은 출력의 예입니다.

![](./images/cli-logs (2).gif)

콘솔에서 서비스 로그를 검토합니다.
 솔루션을 보려면 여기를 확장하십시오.

먼저 콘솔에서 ECS로 이동하고 서비스를 드릴다운하여 자세한 정보를 얻습니다. 보시다시피 서비스 검색 세부 정보, 실행 중인 작업 수 및 로그와 같이 서비스 자체에 대해 수집할 수 있는 많은 정보가 있습니다. 실행 중인 서비스에 대한 로그를 검토하려면 로그 탭을 클릭합니다.

![](./images/ecs-console-service-logs (2).gif)

다음으로 거의 실시간으로 서비스 로그를 검토할 수 있습니다. 1주일까지 시간을 되돌리거나 지난 30초로 드릴다운할 수 있습니다. 아래 예에서는 30초를 선택합니다.콘솔 

![](./images/ecs-console-logs (2).gif)

서비스 확장
수동 확장
 솔루션을 보려면 여기를 확장하십시오.

서비스를 수동으로 확장하려면 다음에서 코드를 수정하면 됩니다. app.py 원하는 개수를 1에서 3으로 변경

```
self.fargate_service = aws_ecs.FargateService(
    self, "CrystalFargateService",
    task_definition=self.fargate_task_def,
    cluster=self.base_platform.ecs_cluster,
    security_group=self.base_platform.services_sec_grp,
    desired_count=3,
    #desired_count=1,
    cloud_map_options=aws_ecs.CloudMapOptions(
        cloud_map_namespace=self.base_platform.sd_namespace,
        name='ecsdemo-crystal'
    )
)
```

코드를 업데이트했으면 변경 사항이 적용되는지 확인하겠습니다.

```
cdk diff
```

출력은 다음과 같아야 합니다.

```
diff-service-count
```

변경 사항이 좋아 보이는 것을 확인했으므로 이제 배포해 보겠습니다.

```
cdk deploy
```

출력은 다음과 같아야 합니다.

![](./images/cdk-deploy-service-count (2).png)

자동 확장
 솔루션을 보려면 여기를 확장하십시오.

곧!

### ecs-cli Fargate 모드

Crystal Backend API를 불러오겠습니다!

크리스탈 애플리케이션 배포:

```
cd ~/environment/ecsdemo-crystal
envsubst < ecs-params.yml.template >ecs-params.yml

ecs-cli compose --project-name ecsdemo-crystal service up \
    --create-log-groups \
    --private-dns-namespace service \
    --enable-service-discovery \
    --cluster-config container-demo \
    --vpc $vpc
```

여기에서 디렉토리를 크리스탈 애플리케이션 코드 디렉토리로 변경합니다. NSenvsubst 명령 템플릿 ecs-params.yml현재 값으로 파일을 만듭니다. 그런 다음 ECS 클러스터에서 크리스탈 서비스를 시작합니다(기본 시작 유형은 Fargate).

참고: ecs-cli는 서비스 검색을 위한 프라이빗 dns 네임스페이스 및 Cloudwatch 로그의 로그 그룹 구축을 처리합니다.

실행 중인 컨테이너를 보고 나중에 사용할 수 있도록 작업 ID의 출력을 env 변수로 저장합니다.

```
ecs-cli compose --project-name ecsdemo-crystal service ps \
    --cluster-config container-demo

task_id=$(ecs-cli compose --project-name ecsdemo-crystal service ps --cluster-config container-demo | awk -F \/ 'FNR == 2 {print $2}')
```

하나의 작업을 등록해야 합니다.

연결 가능성 확인(브라우저에서 URL 열기):

```
alb_url=$(aws cloudformation describe-stacks --stack-name container-demo-alb --query 'Stacks[0].Outputs[?OutputKey==`ExternalUrl`].OutputValue' --output text)
echo "Open $alb_url in your browser"
```

이 명령은 수신 ALB의 URL을 조회하고 출력합니다. 클릭하여 열거나 브라우저에 복사하여 붙여넣을 수 있어야 합니다.

로그 보기:

```
# Referencing task id from above ps command
ecs-cli logs --task-id $task_id \
    --follow --cluster-config container-demo
```

로그를 보려면 이전 ps명령 에서 작업 ID를 찾아 이 명령에서 사용합니다. 작업의 로그를 따를 수도 있습니다.

작업 확장:

```
ecs-cli compose --project-name ecsdemo-crystal service scale 3 \
    --cluster-config container-demo
ecs-cli compose --project-name ecsdemo-crystal service ps \
    --cluster-config container-demo
```

이제 컨테이너가 3개의 가용 영역 모두에 고르게 분산되었음을 알 수 있습니다.

## 컴퓨팅 리소스를 정리합니다.

먼저 컴퓨팅 리소스를 정리하겠습니다.

서비스, ​​ALB, ECS 클러스터, VPC 등으로 시작하여 컴퓨팅 리소스를 삭제합니다.

```
cd ~/environment/ecsdemo-frontend

ecs-cli compose --project-name ecsdemo-crystal service rm --cluster-config container-demo
ecs-cli compose --project-name ecsdemo-nodejs service rm --cluster-config container-demo
ecs-cli compose --project-name ecsdemo-frontend service rm --delete-namespace --cluster-config container-demo

aws cloudformation delete-stack --stack-name container-demo-alb
aws cloudformation wait stack-delete-complete --stack-name container-demo-alb
aws cloudformation delete-stack --stack-name container-demo
```

## 우리는 무엇을 성취했는가
우리는 다음을 가지고 있습니다:

업계 모범 사례를 사용하여 3계층 다중 언어 응용 프로그램 배포
CloudWatchLogs에 대한 각 마이크로 서비스의 중앙 집중식 로그
CloudFormation을 사용하여 코드로서의 인프라 구성
ecs-cli를 사용하여 기본 제공되는 AWS 모범 사례를 사용하여 배포
구성, 관리, 패치 또는 재부팅을 위해 단일 서버를 배포하지 않았습니다!

